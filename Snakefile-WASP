configfile: "config-WASP.yaml"

import glob
from snakemake.utils import min_version

##### set minimum snakemake version #####
min_version("5.1.4")


def read_samples():
    """Function to get names and fastq paths from a sample file specified
    in the configuration. Input file is expected to have 4 columns:
    <1000genomes_id> <unique_sample_id> <fastq1_path> <fastq2_path>. Modify
    this function as needed to provide a dictionary of sample_id keys and
    (fastq1, fastq1) values"""
    f = open(config['sample_file'], "r")
    samp_dict = {}
    for line in f:
        words = line.strip().split("\t")
        samp_dict[words[1]] = (words[2], words[3])
    return samp_dict

SAMP = read_samples()
# change SAMP_NAMES here to contain whichever sample names you'd like
# to generate files for
if 'SAMP_NAMES' not in config:
    config['SAMP_NAMES'] = list(SAMP.keys())


def read_1kg_samples():
    f = open(config['sample_file'], "r")
    samp_dict = {}
    for line in f:
        words = line.strip().split("\t")
        samp_dict[words[1]] = words[0]
    return samp_dict

SAMP_TO_1KG = read_1kg_samples()


rule all:
    input:
        # if you'd like to generate files for only a subset of the samples,
        # you should specify which samples in the SAMP_NAMES variable above
        expand(config['output_dir'] + "/rmdup/{sample}.keep.rmdup.sort.bam",
               sample=config['SAMP_NAMES'])

# run the following to execute the pipeline:
# out_path="/iblm/netapp/home/amassarat/allele_specific_analysis/snakemake/out"; snakemake -s Snakefile-WASP --cluster "qsub -t 1 -V -q iblm.q -j y -o ${out_path}/qout" -j 24 --config output_dir=${out_path} --latency-wait 60 --use-conda >>${out_path}/out 2>&1 &

rule split_vcf_by_chr:
    """Split the provided VCF file by chromosome and gzip it for WASP"""
    input:
        vcf = config['vcf_file']
    output:
        dynamic(config['output_dir'] + "/genotypes/ALL.chr{chr_num}.vcf.gz")
    conda: "envs/default.yaml"
    shell:
        "SnpSift split {input} && "
        "gzip {config[output_dir]}/genotypes/*.vcf"

rule vcf2h5:
    """Convert VCF data files to HDF5 format"""
    input:
        chrom = config['chrom_info'],
        vcfs = rules.split_vcf_by_chr.output
    output:
        snp_index = config['snp_h5_dir'] + "/snp_index.h5",
        snp_tab = config['snp_h5_dir'] + "/snp_tab.h5",
        haplotype = config['snp_h5_dir'] + "/haplotype.h5"
    conda: "envs/pytables2.yaml"
    shell:
        "{config[wasp_dir]}/snp2h5/snp2h5 "
        "  --chrom {input.chrom} "
        "  --format vcf "
        "  --snp_index {output.snp_index} "
        "  --snp_tab {output.snp_tab} "
        "  --haplotype {output.haplotype} "
        "  {input.vcfs}"

rule map_STAR_paired_end1:
    """map reads using STAR"""
    input:
        fastq1 = lambda wildcards: SAMP[wildcards.sample][0],
        fastq2 = lambda wildcards: SAMP[wildcards.sample][1],
        index = config["ref_genome_star"]
    output:
        config["output_dir"] + "/map1/{sample}/Aligned.out.bam"
    params:
        prefix = config["output_dir"] + "/map1/{sample}/"
    conda: "envs/default.yaml"
    shell:
        "STAR --runThreadN {threads} "
            "--genomeDir {input.index} "
            "--readFilesIn {input.fastq1} {input.fastq2} "
            "--readFilesCommand zcat "
            "--outFileNamePrefix {params.prefix}"

rule sort_and_index_bam1:
    """sort and index bam generated by first mapping step"""
    input:
        config["output_dir"] + "/map1/{sample}/Aligned.out.bam"
    output:
        config["output_dir"] + "/map1_sort/{sample}.bam",
        config["output_dir"] + "/map1_sort/{sample}.bam.bai"
    threads: config['num_threads']
    conda: "envs/default.yaml"
    shell:
        "samtools sort -@ {threads} -o {output[0]} {input} && "
        "samtools index -@ {threads} {output[0]}"

rule find_intersecting_snps_paired_end:
    """find intersecting SNPs using WASP script"""
    input:
        bam = config["output_dir"] + "/map1_sort/{sample}.bam",
        snp_index = config["snp_h5_dir"] + "/snp_index.h5",
        snp_tab = config["snp_h5_dir"] + "/snp_tab.h5",
        haplotype = config['snp_h5_dir'] + "/haplotype.h5"
    params:
        sample_names = lambda wildcards: SAMP_TO_1KG[wildcards.sample]
    output:
        fastq1 = config["output_dir"] + "/find_intersecting_snps/{sample}.remap.fq1.gz",
        fastq2 = config["output_dir"] + "/find_intersecting_snps/{sample}.remap.fq2.gz",
        keep_bam = config["output_dir"] + "/find_intersecting_snps/{sample}.keep.bam",
        remap_bam = config["output_dir"] + "/find_intersecting_snps/{sample}.to.remap.bam"
    conda: "envs/pytables2.yaml"
    shell:
        "python {config[wasp_dir]}/mapping/find_intersecting_snps.py "
        "    --is_paired_end "
        "    --is_sorted "
        "    --output_dir {config[output_dir]}/find_intersecting_snps "
        "    --snp_tab {input.snp_tab} "
        "    --snp_index {input.snp_index} "
        "    --haplotype {input.haplotype} "
        "    --samples {params.sample_names} "
        "    {input.bam}"

rule map_STAR_paired_end2:
    """map reads a second time using STAR"""
    input:
        fastq1 = config['output_dir'] + "/find_intersecting_snps/{sample}.remap.fq1.gz",
        fastq2 = config['output_dir'] + "/find_intersecting_snps/{sample}.remap.fq2.gz",
        index = config["ref_genome_star"]
    output:
        config["output_dir"] + "/map2/{sample}/Aligned.out.bam"
    params:
        prefix = config["output_dir"] + "/map2/{sample}/"
    threads: config['num_threads']
    conda: "envs/default.yaml"
    shell:
        "STAR --runThreadN {threads} "
            "--genomeDir {input.index} "
            "--readFilesIn {input.fastq1} {input.fastq2} "
            "--readFilesCommand zcat "
            "--outFileNamePrefix {params.prefix}"

rule sort_and_index_bam2:
    """sort and index bam generated by second mapping step"""
    input:
        config["output_dir"] + "/map2/{sample}/Aligned.out.bam"
    output:
        config["output_dir"] + "/map2_sort/{sample}.bam",
        config["output_dir"] + "/map2_sort/{sample}.bam.bai"
    threads: config['num_threads']
    conda: "envs/default.yaml"
    shell:
        "samtools sort -@ {threads} -o {output[0]} {input} && "
        "samtools index -@ {threads} {output[0]}"

rule filter_remapped_reads:
    """filter reads from second mapping step"""
    input:
        to_remap_bam = config['output_dir'] + "/find_intersecting_snps/{sample}.to.remap.bam",
        remap_bam = config['output_dir'] + "/map2_sort/{sample}.bam",
    output:
        keep_bam = config['output_dir'] + "/filter_remapped_reads/{sample}.keep.bam"
    conda: "envs/pytables2.yaml"
    shell:
        "python {config[wasp_dir]}/mapping/filter_remapped_reads.py "
        "  {input.to_remap_bam} {input.remap_bam} {output.keep_bam}"

rule sort_filtered_bam:
    """sort 'keep' BAM file from rules.filter_remapped_reads for rules.rmdup_pe"""
    input:
        config['output_dir'] + "/filter_remapped_reads/{sample}.keep.bam"
    output:
        config['output_dir'] + "/filter_remapped_reads/{sample}.keep.sort.bam"
    threads: config['num_threads']
    conda: "envs/default.yaml"
    shell:
        "samtools sort -@ {threads} -o {output} {input} && "
        "samtools index -@ {threads} {output}"

rule rmdup_pe:
    """remove duplicate read pairs"""
    input:
        config['output_dir'] + "/filter_remapped_reads/{sample}.keep.sort.bam"
    output:
        rmdup = config['output_dir'] + "/rmdup/{sample}.keep.rmdup.bam",
        sort = config['output_dir'] + "/rmdup/{sample}.keep.rmdup.sort.bam"
    threads: config['num_threads']
    conda: "envs/pytables2.yaml"
    shell:
        "python {config[wasp_dir]}/mapping/rmdup_pe.py {input} {output.rmdup} && "
        "samtools sort -@ {threads} -o {output.sort} {output.rmdup} && "
        "samtools index -@ {threads} {output.sort}"
