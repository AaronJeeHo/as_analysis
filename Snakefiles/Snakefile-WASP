import glob
from snakemake.utils import min_version

##### set minimum snakemake version #####
min_version("5.1.4")


# check existence of SAMP variable. it may have already been defined if this
# Snakefile is being included from somewhere else
if 'SAMP2' not in globals():
    def read_samples():
        """Function to get names and fastq paths from a sample file specified
        in the configuration. Input file is expected to have 4 columns:
        <vcf_sample_id> <unique_sample_id> <fastq1_path> <fastq2_path>. Modify
        this function as needed to provide a dictionary of sample_id keys and
        (fastq1, fastq1) values"""
        f = open(config['sample_file'], "r")
        samp_dict = {}
        for line in f:
            words = line.strip().split("\t")
            samp_dict[words[1]] = (words[2], words[3])
        return samp_dict
    SAMP2 = read_samples()

# the user can change config['SAMP_NAMES'] here (or define it in the config
# file) to contain whichever sample names they'd like to run the pipeline on
if 'SAMP_NAMES' not in config:
    config['SAMP_NAMES'] = list(SAMP2.keys())

# check existence of SAMP_TO_VCF_ID. it may have already been defined if this
# Snakefile is being included from somewhere else
if 'SAMP_TO_VCF_ID' not in globals():
    def read_vcf_samples():
        f = open(config['sample_file'], "r")
        samp_dict = {}
        for line in f:
            words = line.strip().split("\t")
            if words[1] in config['SAMP_NAMES']:
                samp_dict[words[1]] = words[0]
        return samp_dict
    SAMP_TO_VCF_ID = read_vcf_samples()

# set the wasp_dir if the user hasn't specified one
if 'wasp_dir' not in config:
    config['wasp_dir'] = ".snakemake/WASP"
# set the default SNP H5 dir if the user hasn't specified one
if 'snp_h5_dir' not in config:
    config['snp_h5_dir'] = "/genotypes/snp_h5"


if not hasattr(rules, 'all'):
    rule all:
        # if you'd like to run the pipeline on only a subset of the samples,
        # you should specify them in the config['SAMP_NAMES'] variable above
        input:
            expand(config['output_dir'] + "/rmdup/{sample}.keep.rmdup.sort.bam",
                   sample=config['SAMP_NAMES'])

rule get_WASP:
    """Download WASP if doesn't exist"""
    output:
        find_intersecting_snps_script = config['wasp_dir'] + "/mapping/find_intersecting_snps.py",
        filter_remapped_reads_script = config['wasp_dir'] + "/mapping/filter_remapped_reads.py",
        rmdup_script = config['wasp_dir'] + "/mapping/rmdup_pe.py",
        bam2h5_script = config['wasp_dir'] + "/CHT/bam2h5.py",
        makefile = config['wasp_dir'] + "/snp2h5/Makefile"
    conda: "../envs/default.yaml"
    shell:
        "[ ! -d \"{config[wasp_dir]}\"] && "
        "curl -Ls https://api.github.com/repos/bmvdgeijn/WASP/tarball | "
        "tar zxf - -C \"{config[wasp_dir]}\" --strip-components 1"

rule install_WASP:
    """Make WASP snp2h5 if it hasn't been compiled yet"""
    input:
        rules.get_WASP.output.makefile
    output:
        snp2h5_script = config['wasp_dir'] + "/snp2h5/snp2h5"
    conda: "../envs/default.yaml"
    shell:
        "conda_path=\"$CONDA_PREFIX\" && "
        "if [ -z $conda_path ]; then "
        "conda_path=\"$(dirname $(dirname $(which conda)))\"; "
        "fi && [ ! -z $conda_path ] && "
        "(cd \"{config[wasp_dir]}/snp2h5\" && "
        "make -s HDF_INSTALL=\"$conda_path\")"

rule split_vcf_by_chr:
    """Split the provided VCF file by chromosome and gzip it for WASP"""
    input:
        vcf = config['vcf_file'],
        vcf_index = config['vcf_file'] + ".tbi"
    output:
        dynamic(config['output_dir'] + "/genotypes/ALL.{chr_num}.vcf.gz")
    conda: "../envs/default.yaml"
    shell:
        "SnpSift split {input.vcf} && "
        "gzip {config[output_dir]}/genotypes/*.vcf"

rule vcf2h5:
    """Convert VCF data files to HDF5 format"""
    input:
        chrom = config['chrom_info'],
        vcfs = rules.split_vcf_by_chr.output,
        snp2h5_script = rules.install_WASP.output.snp2h5_script
    output:
        snp_index = config['snp_h5_dir'] + "/snp_index.h5",
        snp_tab = config['snp_h5_dir'] + "/snp_tab.h5",
        haplotype = config['snp_h5_dir'] + "/haplotype.h5"
    conda: "../envs/default.yaml"
    shell:
        "{input.snp2h5_script} "
            "--chrom {input.chrom} "
            "--format vcf "
            "--snp_index {output.snp_index} "
            "--snp_tab {output.snp_tab} "
            "--haplotype {output.haplotype} "
            "{input.vcfs}"

rule map_STAR_paired_end1:
    """map reads using STAR"""
    input:
        fastq1 = lambda wildcards: SAMP2[wildcards.sample][0],
        fastq2 = lambda wildcards: SAMP2[wildcards.sample][1],
        index = config['ref_genome_star']
    params:
        prefix = config['output_dir'] + "/map1/{sample}/"
    output:
        temp(config['output_dir'] + "/map1/{sample}/Aligned.out.bam")
    threads: config['num_threads']
    conda: "../envs/default.yaml"
    shadow: "minimal"
    shell:
        "STAR --runThreadN {threads} "
            "--genomeDir {input.index} "
            "--readFilesIn {input.fastq1} {input.fastq2} "
            "--readFilesCommand zcat "
            "--outSAMtype BAM Unsorted "
            "--alignEndsType EndToEnd "
            "--outFileNamePrefix {params.prefix}"

rule sort_and_index_bam1:
    """sort and index bam generated by first mapping step"""
    input:
        rules.map_STAR_paired_end1.output
    output:
        bam = config['output_dir'] + "/map1_sort/{sample}.bam",
        index = config['output_dir'] + "/map1_sort/{sample}.bam.bai"
    threads: config['num_threads']
    conda: "../envs/default.yaml"
    shell:
        "samtools sort -@ {threads} -o {output.bam} {input} && "
        "samtools index -@ {threads} {output.bam} && "
        "touch -c {output.bam}"

rule find_intersecting_snps_paired_end:
    """find intersecting SNPs using WASP script"""
    input:
        bam = rules.sort_and_index_bam1.output.bam,
        snp_index = rules.vcf2h5.output.snp_index,
        snp_tab = rules.vcf2h5.output.snp_tab,
        haplotype = rules.vcf2h5.output.haplotype,
        find_intersecting_snps_script = rules.get_WASP.output.find_intersecting_snps_script
    params:
        sample_names = lambda wildcards: SAMP_TO_VCF_ID[wildcards.sample]
    output:
        fastq1 = config['output_dir'] + "/find_intersecting_snps/{sample}.remap.fq1.gz",
        fastq2 = config['output_dir'] + "/find_intersecting_snps/{sample}.remap.fq2.gz",
        keep_bam = temp(config['output_dir'] + "/find_intersecting_snps/{sample}.keep.bam"),
        remap_bam = config['output_dir'] + "/find_intersecting_snps/{sample}.to.remap.bam"
    conda: "../envs/default.yaml"
    shell:
        "python {input.find_intersecting_snps_script} "
            "--is_paired_end "
            "--is_sorted "
            "--output_dir {config[output_dir]}/find_intersecting_snps "
            "--snp_tab {input.snp_tab} "
            "--snp_index {input.snp_index} "
            "--haplotype {input.haplotype} "
            "--samples {params.sample_names} "
            "{input.bam}"

rule map_STAR_paired_end2:
    """map reads a second time using STAR"""
    input:
        fastq1 = rules.find_intersecting_snps_paired_end.output.fastq1,
        fastq2 = rules.find_intersecting_snps_paired_end.output.fastq2,
        index = config['ref_genome_star']
    params:
        prefix = config['output_dir'] + "/map2/{sample}/"
    output:
        temp(config['output_dir'] + "/map2/{sample}/Aligned.out.bam")
    threads: config['num_threads']
    conda: "../envs/default.yaml"
    shadow: "minimal"
    shell:
        "STAR --runThreadN {threads} "
            "--genomeDir {input.index} "
            "--readFilesIn {input.fastq1} {input.fastq2} "
            "--readFilesCommand zcat "
            "--outSAMtype BAM Unsorted "
            "--alignEndsType EndToEnd "
            "--outFileNamePrefix {params.prefix}"

rule sort_and_index_bam2:
    """sort and index bam generated by second mapping step"""
    input:
        rules.map_STAR_paired_end2.output
    output:
        bam = config['output_dir'] + "/map2_sort/{sample}.bam",
        index = config['output_dir'] + "/map2_sort/{sample}.bam.bai"
    threads: config['num_threads']
    conda: "../envs/default.yaml"
    shell:
        "samtools sort -@ {threads} -o {output.bam} {input} && "
        "samtools index -@ {threads} {output.bam} && "
        "touch -c {output.bam}"

rule filter_remapped_reads:
    """filter reads from second mapping step"""
    input:
        to_remap_bam = rules.find_intersecting_snps_paired_end.output.remap_bam,
        remap_bam = rules.sort_and_index_bam2.output.bam,
        filter_remapped_reads_script = rules.get_WASP.output.filter_remapped_reads_script
    output:
        temp(config['output_dir'] + "/filter_remapped_reads/{sample}.keep.bam")
    conda: "../envs/default.yaml"
    shell:
        "python {input.filter_remapped_reads_script} "
        "{input.to_remap_bam} {input.remap_bam} {output}"

rule sort_filtered_bam:
    """sort 'keep' BAM file from rules.filter_remapped_reads for rules.rmdup_pe"""
    input:
        rules.filter_remapped_reads.output
    output:
        config['output_dir'] + "/filter_remapped_reads/{sample}.keep.sort.bam"
    threads: config['num_threads']
    conda: "../envs/default.yaml"
    shell:
        "samtools sort -@ {threads} -o {output} {input} && "
        "samtools index -@ {threads} {output}"

rule rmdup_pe:
    """remove duplicate read pairs"""
    input:
        bam = rules.sort_filtered_bam.output,
        rmdup_script = rules.get_WASP.output.rmdup_script
    output:
        rmdup = temp(config['output_dir'] + "/rmdup/{sample}.keep.rmdup.bam"),
        sort = config['output_dir'] + "/rmdup/{sample}.keep.rmdup.sort.bam"
    threads: config['num_threads']
    conda: "../envs/default.yaml"
    shell:
        "python {input.rmdup_script} {input.bam} {output.rmdup} && "
        "samtools sort -@ {threads} -o {output.sort} {output.rmdup} && "
        "samtools index -@ {threads} {output.sort} && "
        "touch -c {output.sort}"
